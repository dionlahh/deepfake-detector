{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "In this notebook, we trained 4 different EfficientNet models on the faces extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/_1k_wx3s2fg5c_7xkb973_4h0000gn/T/ipykernel_17924/294687284.py:25: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats(\"svg\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.display import set_matplotlib_formats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "from common.model_utils import build_model, input_size\n",
    "from common.landmark_model_utils import build_model_landmarks\n",
    "\n",
    "# import packages and modules for graph plotting\n",
    "from pandas.plotting import table\n",
    "\n",
    "# setup output image format (Chrome works best)\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "class ClearMemory(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        gc.collect()\n",
    "        k.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/6n/_1k_wx3s2fg5c_7xkb973_4h0000gn/T/ipykernel_17924/263659865.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU is available and in use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 15:48:48.127926: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-01-16 15:48:48.127941: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-01-16 15:48:48.127944: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-01-16 15:48:48.127977: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-16 15:48:48.127993: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available and in use.\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create necessary project folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Landmark Models' already exists at ./Landmark Models\n",
      "Folder 'Landmark History' already exists at ./Landmark History\n",
      "Folder 'Result' already exists at ./Result\n",
      "Folder 'Tuning' already exists at ./Tuning\n"
     ]
    }
   ],
   "source": [
    "# Input list of folder names\n",
    "models_folder = \"Landmark Models\"\n",
    "history_folder = \"Landmark History\"\n",
    "result_folder = \"Result\"\n",
    "tuning_folder = \"Tuning\"\n",
    "data_folder = \"Training Landmarks Data\"\n",
    "folder_names = [models_folder, history_folder, result_folder, tuning_folder]\n",
    "\n",
    "for folder_name in folder_names:\n",
    "    folder_path = os.path.join(\".\", folder_name)\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "        print(f\"Folder '{folder_name}' created at {folder_path}\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder_name}' already exists at {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory already exists!\n"
     ]
    }
   ],
   "source": [
    "# # Split Data\n",
    "# # ├── Test\n",
    "# # │   ├── manipulated\n",
    "# # │   └── original\n",
    "# # ├── Training\n",
    "# # │   ├── manipulated\n",
    "# # │   └── original\n",
    "# # └── Validation\n",
    "# #     ├── manipulated\n",
    "# #     └── original\n",
    "\n",
    "# # Check if the dataset directory is already present to avoid redundant read and writes\n",
    "# isExist = os.path.exists(\"./\" + data_folder + \"/\")\n",
    "\n",
    "# if not isExist:\n",
    "#     # Creates the appropriate directory structures for training, validation and test sets.\n",
    "#     try:\n",
    "#         shutil.rmtree(\"./\" + data_folder + \"/\")\n",
    "#     except:\n",
    "#         pass  # Split Data didn't exist\n",
    "\n",
    "#     os.mkdir(\"./Split Data\")\n",
    "#     cdf = {\n",
    "#         \"Training\": 0.7,\n",
    "#         \"Validation\": 0.85,\n",
    "#         \"Test\": 1,\n",
    "#     }  # OBS! Has to be increment percentages of 5 to make batch size fit\n",
    "#     for dir in list(cdf.keys()):\n",
    "#         os.mkdir(\"./\" + data_folder + \"/{}\".format(dir))\n",
    "#         os.mkdir(\"./\" + data_folder + \"/{}/manipulated\".format(dir))\n",
    "#         os.mkdir(\"./\" + data_folder + \"/{}/original\".format(dir))\n",
    "# else:\n",
    "#     print(\"Dataset directory already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models we want to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [62856], output_shape = [62720]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 62856), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[1;32m     12\u001b[0m     size \u001b[38;5;241m=\u001b[39m input_size(model_type)\n\u001b[0;32m---> 13\u001b[0m     models[model_type] \u001b[38;5;241m=\u001b[39m build_model_landmarks(model_type\u001b[38;5;241m=\u001b[39m model_type, input_shape\u001b[38;5;241m=\u001b[39m(size, size, \u001b[38;5;241m3\u001b[39m), landmarks_shape\u001b[38;5;241m=\u001b[39mlandmarks_shape)\n",
      "File \u001b[0;32m~/Github/Y4_Projects/FYP/dev/common/landmark_model_utils.py:83\u001b[0m, in \u001b[0;36mbuild_model_landmarks\u001b[0;34m(model_type, input_shape, landmarks_shape)\u001b[0m\n\u001b[1;32m     80\u001b[0m concatenated_output \u001b[38;5;241m=\u001b[39m Concatenate(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)([flattened_output, flattened_landmarks])\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Reshape to match the original flattened EfficientNet output shape\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m reshaped_output \u001b[38;5;241m=\u001b[39m Reshape((\u001b[38;5;241m62720\u001b[39m,))(concatenated_output)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Add additional Dense layers if needed\u001b[39;00m\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/fyp/lib/python3.11/site-packages/keras/src/layers/reshaping/reshape.py:118\u001b[0m, in \u001b[0;36mReshape._fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    116\u001b[0m     output_shape[unknown] \u001b[38;5;241m=\u001b[39m original \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m known\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original \u001b[38;5;241m!=\u001b[39m known:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"reshape_1\" (type Reshape).\n\ntotal size of new array must be unchanged, input_shape = [62856], output_shape = [62720]\n\nCall arguments received by layer \"reshape_1\" (type Reshape):\n  • inputs=tf.Tensor(shape=(None, 62856), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "    \"EfficientNetB0\",\n",
    "    \"EfficientNetB1\",\n",
    "    \"EfficientNetB2\",\n",
    "    \"EfficientNetB3\",\n",
    "]\n",
    "\n",
    "models = {}\n",
    "landmarks_shape = (68, 2)\n",
    "\n",
    "for model_type in model_list:\n",
    "    size = input_size(model_type)\n",
    "    models[model_type] = build_model_landmarks(model_type= model_type, input_shape=(size, size, 3), landmarks_shape=landmarks_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import efficientnet as efn\n",
    "\n",
    "# Specify the path to the train and validation data\n",
    "trainpath = os.path.join(\".\", data_folder, \"Training\")\n",
    "valpath = os.path.join(\".\", data_folder, \"Validation\")\n",
    "\n",
    "# constant fields\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "models_fit_history = {}\n",
    "models_result = {}\n",
    "models_failed = []\n",
    "\n",
    "# Train the models\n",
    "for model_type in model_list:\n",
    "    error_flag = False\n",
    "    imgsize = input_size(model_type)\n",
    "\n",
    "    # Check if the model is already trained to avoid redundant processing\n",
    "    if not os.path.exists(os.path.join(\".\", models_folder, model_type + \".hdf5\")):\n",
    "        # Load data and preprocess it\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            preprocessing_function=efn.preprocess_input\n",
    "        )\n",
    "\n",
    "        train_gen = train_datagen.flow_from_directory(\n",
    "            trainpath,\n",
    "            target_size=(imgsize, imgsize),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "        val_gen = train_datagen.flow_from_directory(\n",
    "            valpath,\n",
    "            target_size=(imgsize, imgsize),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode=\"categorical\",\n",
    "        )\n",
    "\n",
    "        # load model\n",
    "        model = models[model_type]\n",
    "        model.summary()\n",
    "\n",
    "        # callbacks for early stopping and saving the best model so far based on validation loss for each epoch\n",
    "        cb_early_stopper = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "        cb_checkpointer = ModelCheckpoint(\n",
    "            filepath=os.path.join(\".\", models_folder, model_type + \".hdf5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            mode=\"auto\",\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # train model\n",
    "            fit_history = model.fit(\n",
    "                train_gen,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=val_gen,\n",
    "                callbacks=[\n",
    "                    cb_checkpointer,\n",
    "                    cb_early_stopper,\n",
    "                    ClearMemory(),\n",
    "                ],  # ClearMemory() is a custom callback to clear memory after each epoch to avoid memory leak\n",
    "            )\n",
    "\n",
    "            models_fit_history[model_type] = fit_history\n",
    "        except Exception as e:\n",
    "            models_failed.append(model_type)\n",
    "            error_flag = True\n",
    "            print(\"Encountered error while training:\", e)\n",
    "\n",
    "        history_df = pd.DataFrame(models_fit_history[model_type].history)\n",
    "        os.path.join(\".\", history_folder, model_type + \".hdf5\")\n",
    "        history_df.to_csv(\n",
    "            os.path.join(\".\", history_folder, model_type + \"_history.csv\"), index=False\n",
    "        )\n",
    "\n",
    "    # Load the model from disk if it is already trained\n",
    "    else:\n",
    "        imgsize = input_size(model_type)\n",
    "        model = build_model(model_type, (imgsize, imgsize, 3))\n",
    "        model_path = os.path.join(\".\", models_folder, model_type + \".hdf5\")\n",
    "        model.load_weights(model_path)\n",
    "        print(\"Loaded model {} from disk\".format(model_type))\n",
    "\n",
    "    # Skip testing if there was an error in training the model\n",
    "    if error_flag:\n",
    "        continue\n",
    "\n",
    "    gc.collect()\n",
    "    k.clear_session()\n",
    "\n",
    "if len(models_failed) == 0:\n",
    "    print(\"Models that failed training:\")\n",
    "    for fail in models_failed:\n",
    "        print(\"\\t\" + fail)\n",
    "else:\n",
    "    print(\"All models have been initialised!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the test data\n",
    "test_path = os.path.join(\".\", data_folder, \"Test\")\n",
    "\n",
    "# Test the models\n",
    "for model_type in model_list:\n",
    "    print(\"Starting prediction for\", model_type)\n",
    "    test_img_gen = ImageDataGenerator(\n",
    "        preprocessing_function=efn.preprocess_input\n",
    "    )\n",
    "\n",
    "    imgsize = input_size(model_type)\n",
    "\n",
    "    test_generator = test_img_gen.flow_from_directory(\n",
    "        directory=test_path,\n",
    "        target_size=(imgsize, imgsize),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        seed=123,\n",
    "    )\n",
    "\n",
    "    model = build_model(model_type,(imgsize, imgsize, 3))\n",
    "    model_path = os.path.join(\".\", models_folder, model_type + \".hdf5\")\n",
    "    model.load_weights(model_path)\n",
    "    print(\"Loaded model {} from disk\".format(model_type))\n",
    "\n",
    "    predicted = model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "    predicted_class_indices = np.argmax(predicted, axis=1)\n",
    "    y = test_generator.classes\n",
    "\n",
    "    Accuracy = metrics.accuracy_score(y, predicted_class_indices)\n",
    "    print(\"Model Accuracy: \", Accuracy)\n",
    "\n",
    "    Precision = metrics.precision_score(y, predicted_class_indices, average=\"binary\")\n",
    "    print(\"Model Precision: \", Precision)\n",
    "\n",
    "    Recall = metrics.recall_score(y, predicted_class_indices, average=\"binary\")\n",
    "    print(\"Model Recall: \", Recall)\n",
    "\n",
    "    Auc = metrics.roc_auc_score(y, predicted_class_indices)\n",
    "    print(\"AUC: \", Auc, \"\\n\")\n",
    "\n",
    "    models_result[model_type] = {\n",
    "        \"Accuracy\": Accuracy,\n",
    "        \"Precision\": Precision,\n",
    "        \"Recall\": Recall,\n",
    "        \"AUC\": Auc,\n",
    "    }\n",
    "\n",
    "    filenames = [os.path.split(i)[1] for i in test_generator.filenames]\n",
    "    actualLabel = [os.path.split(i)[0] for i in test_generator.filenames]\n",
    "\n",
    "    for i in range(len(actualLabel)):\n",
    "        if actualLabel[i] == \"manipulated\":\n",
    "            actualLabel[i] = \"0\"\n",
    "        else:\n",
    "            actualLabel[i] = \"1\"\n",
    "    results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": pd.Series(filenames),\n",
    "            \"actual label\": pd.Series(actualLabel),\n",
    "            \"pred label\": pd.Series(predicted_class_indices),\n",
    "        }\n",
    "    )\n",
    "    os.makedirs(result_folder, exist_ok=True)\n",
    "    results_df.to_csv(os.path.join(\".\", result_folder, model_type + \"_out.csv\"))\n",
    "\n",
    "with open(os.path.join(\".\", result_folder, \"model_results.json\"), \"w\") as outfile:\n",
    "    json.dump(models_result, outfile)\n",
    "\n",
    "model_prediction_results = pd.DataFrame.from_dict(models_result, orient=\"index\")\n",
    "model_prediction_results.to_csv(\n",
    "    os.path.join(\".\", result_folder, \"prediction_results.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a 2 dimensional dataframe out of the given data\n",
    "results_df = pd.read_csv(os.path.join(\".\", result_folder, \"prediction_results.csv\"))\n",
    "results_df = results_df.rename(columns={\"Unnamed: 0\": \"Model\"})\n",
    "\n",
    "\n",
    "# Set the model names as the index for better labeling on the x-axis\n",
    "results_df.set_index(\"Model\", inplace=True)\n",
    "\n",
    "results_df_transposed = results_df.transpose()\n",
    "\n",
    "# Create a bar plot for the DataFrame\n",
    "ax = results_df_transposed.plot(kind=\"bar\", figsize=(12, 8))\n",
    "ax.set_ylabel(\"Scores\")\n",
    "# ax.set_xlabel('Metrics')\n",
    "ax.set_title(\"Model Evaluation Metrics\")\n",
    "\n",
    "# Set the y-axis lower limit to 0.90\n",
    "ax.set_ylim(0.70, 1.0)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.25, 1), ncol=1)\n",
    "\n",
    "# Show the plot\n",
    "plt.grid()\n",
    "\n",
    "# Bold the fonts in the graph\n",
    "matplotlib.rcParams[\"font.weight\"] = \"bold\"\n",
    "matplotlib.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "matplotlib.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "ax.tick_params(axis=\"y\", which=\"major\", labelsize=11, width=2, length=6)\n",
    "\n",
    "# plt.xticks(rotation=45)\n",
    "ax.set_xticklabels([])\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "table(ax, results_df.round(4), loc=\"bottom\", cellLoc=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
